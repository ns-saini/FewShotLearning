{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "outputs": [],
      "source": [
        "import  torch.nn.functional as F\n",
        "def train(model, device, train_loader, optimizer,  epoch,criterion=F.cross_entropy, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8GlJkOdfYY0"
      },
      "source": [
        "***Challenge 2***\n",
        "\n",
        "You may use the same testbed but without the constraints on external datasets or models trained on exeternal datasets. You may not however use any of the CIFAR-10 training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.311314\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.813360\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.163491\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.740979\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.532407\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.421177\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.354255\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.303537\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.258603\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.220844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nishant/anaconda3/envs/comp691/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.4800, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.553606\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.000267\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.243051\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.744609\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.513484\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.382735\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.295237\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.235535\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.191058\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.154596\n",
            "\n",
            "Test set: Average loss: 0.4026, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.439381\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.891225\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.151048\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.657312\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.420039\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.293763\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.214692\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.161222\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.123850\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.097176\n",
            "\n",
            "Test set: Average loss: 0.3778, Accuracy: 386/400 (96.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.343940\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.870976\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.234115\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.798905\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.582853\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.469155\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.398047\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.344488\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.299873\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.262850\n",
            "\n",
            "Test set: Average loss: 0.4608, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.341888\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.864664\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.230109\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 0.799964\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 0.580423\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.459892\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 0.383606\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 0.323528\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 0.270817\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 0.228042\n",
            "\n",
            "Test set: Average loss: 0.4924, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "Acc over 5 instances: 86.20 +- 5.59\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224)\n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "\n",
        "accs = []\n",
        "\n",
        "# r_model = torchvision.models.resnet18(pretrained=True)    \n",
        "\n",
        "for seed in range( 5):\n",
        "    prng = RandomState(seed)\n",
        "    random_permute = prng.permutation(np.arange(0, 5000))\n",
        "    classes =  prng.permutation(np.arange(0,10))\n",
        "    indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "    indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "    train_data = Subset(cifar_data, indx_train)\n",
        "    val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "    print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                                batch_size=128,\n",
        "                                                shuffle=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                            batch_size=128,\n",
        "                                            shuffle=False)\n",
        "\n",
        "\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    # model = r_model\n",
        "    # model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "    model.classifier = nn.Linear(960, 10)\n",
        "    \n",
        "    optimizer = torch.optim.SGD(model.classifier.parameters(),\n",
        "                                lr=0.01, momentum=0.9,\n",
        "                                weight_decay=0.0005)\n",
        "    model.to(device)\n",
        "    for epoch in range(10):\n",
        "        train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "\n",
        "    accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "InceptionV4\n",
            "seed 0\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.255103\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.377796\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.148120\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.080988\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.052742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nishant/anaconda3/envs/comp691/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5619, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "seed 1\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 5.391768\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.440566\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.229436\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.101771\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.057886\n",
            "\n",
            "Test set: Average loss: 0.4060, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "seed 2\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 3.150031\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.361000\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.091763\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.051889\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.035227\n",
            "\n",
            "Test set: Average loss: 0.2653, Accuracy: 389/400 (97.25%)\n",
            "\n",
            "seed 3\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 5.043161\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.715670\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.129125\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.061552\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.040006\n",
            "\n",
            "Test set: Average loss: 0.5515, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "seed 4\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 5.730665\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 2.051457\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.181897\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.075047\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.046753\n",
            "\n",
            "Test set: Average loss: 0.5213, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "Acc over 5 instances: 85.10 +- 7.51 , time: 53.89\n",
            "Acc over 5 instances: 85.10 +- 7.51 , time:  53.89 +- 4.02\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.benchmark = True\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224)\n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "\n",
        "accs = []\n",
        "\n",
        "resnext = timm.create_model('resnext26ts', pretrained=True, num_classes=10)\n",
        "efficient_net = timm.create_model('tf_efficientnet_b0', pretrained=True, num_classes=10)\n",
        "resnet18 = timm.create_model('resnet18', pretrained=True, num_classes=10)\n",
        "mobilenet_v3_large = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=10)\n",
        "cait = timm.create_model('cait_xxs24_224', pretrained=True, num_classes=10)\n",
        "edgenext = timm.create_model('edgenext_xx_small', pretrained=True, num_classes=10)\n",
        "inception_v4 = timm.create_model('inception_v4', pretrained=True, num_classes=10)\n",
        "classfiers = [ inception_v4]\n",
        "\n",
        "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
        "from timm.optim import Lion, AdamW, SGDP\n",
        "\n",
        "def run(model):\n",
        "    print(model.__class__.__name__)\n",
        "    accs = np.array([])\n",
        "    times = np.array([])\n",
        "    losses = np.array([])   \n",
        "    model.to(device)\n",
        "    optimizer = SGDP(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
        "    \n",
        "    scheduler = CosineLRScheduler(optimizer, t_initial=5)\n",
        "    for seed in range(5):\n",
        "        print(\"seed\", seed)\n",
        "        prng = RandomState(seed)\n",
        "        random_permute = prng.permutation(np.arange(0, 5000))\n",
        "        classes =  prng.permutation(np.arange(0,10))\n",
        "        indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "        train_data = Subset(cifar_data, indx_train)\n",
        "        val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "        print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                                    batch_size=128,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                                batch_size=128,\n",
        "                                                shuffle=False)\n",
        "        \n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        start.record()\n",
        "        for epoch in range(50):\n",
        "            loss = train(model, device, train_loader, optimizer, epoch, display=epoch%10==0)\n",
        "            losses = np.append(losses, loss)\n",
        "            scheduler.step(loss)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        time = start.elapsed_time(end)\n",
        "        times = np.append(times, time)        \n",
        "        acc = test(model, device, val_loader)\n",
        "        accs = np.append(accs, acc)\n",
        "    print('Acc over 5 instances: %.2f +- %.2f , time: %.2f'%(accs.mean(),accs.std(), times.mean()/1000))    \n",
        "    return accs, times, losses\n",
        "\n",
        "metrics_map = {}\n",
        "for model in classfiers:\n",
        "    accs, times, losses = run(model)\n",
        "    print('Acc over 5 instances: %.2f +- %.2f , time:  %.2f +- %.2f'%(accs.mean(),accs.std(), times.mean()/1000, times.std()/1000))\n",
        "    metrics_map[model.__class__.__name__] = (accs, times, losses)\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('metrics_map-sgd-noscheduler.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics_map, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
