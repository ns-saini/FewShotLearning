{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_pytorch.vit_for_small_dataset import ViT\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from numpy.random import RandomState\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "n_workers = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cifar_data = CIFAR10(root=\"data\", train=True, download=True,  transform= transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.GaussianBlur(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "        ]))\n",
    "\n",
    "cifar_data_val = CIFAR10(root='.',train=True, transform= transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(image_size),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "        ]), download=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_model(patch_size=16, dim=512, depth=3, heads=16):\n",
    "    return ViT(\n",
    "        image_size = 32,\n",
    "        patch_size = patch_size,\n",
    "        num_classes = 10,\n",
    "        dim = dim,\n",
    "        depth = depth,\n",
    "        heads = heads,\n",
    "        mlp_dim = 1024,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    )\n",
    "\n",
    "def training_epoch_transformer(\n",
    "    model, data_loader, optimizer, criterion\n",
    "):\n",
    "    all_loss = []\n",
    "    accuracies = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (\n",
    "        images, labels\n",
    "    ) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        classification_scores = model(images.to(device))\n",
    "        \n",
    "        \n",
    "        loss = criterion(classification_scores, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy = (classification_scores.argmax(dim=1) == labels.to(device)).float().mean()\n",
    "        accuracies.append(accuracy.item())\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        # tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss), all_loss, accuracies\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model, data_loader,\n",
    "):\n",
    "    accuracies = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (\n",
    "        images, labels\n",
    "    ) in enumerate(data_loader):\n",
    "        model.eval()\n",
    "        classification_scores = model(images.to(device))\n",
    "        \n",
    "        accuracy = (classification_scores.argmax(dim=1) == labels.to(device)).float().mean()\n",
    "        accuracies.append(accuracy.item())\n",
    "\n",
    "    return  accuracies\n",
    "\n",
    "def get_dataloader_transformer(dataset,dataset_val, n_workers, seed, batch_size=128):\n",
    "    prng = RandomState(seed)\n",
    "    random_permute = prng.permutation(np.arange(0, 500))\n",
    "    classes =  prng.permutation(np.arange(0,10))\n",
    "    indx_train = np.concatenate([np.where(np.array(dataset.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
    "    indx_val = np.concatenate([np.where(np.array(dataset_val.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
    "    train_targets = np.array(dataset.targets)[indx_train]\n",
    "    val_targets = np.array(dataset.targets)[indx_val]\n",
    "    train_data = Subset(cifar_data, indx_train)\n",
    "    val_data = Subset(cifar_data_val, indx_val)\n",
    "        \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True,\n",
    "    )    \n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"patch\": 4, \"dim\": 512, \"depth\": 3, \"head\": 4}\n",
      "Epoch 0 - Loss: 2.4203736782073975, lr: 0.00015750147366051494\n",
      "Epoch 10 - Loss: 1.0317171812057495, lr: 0.00026957151786797986\n",
      "Epoch 20 - Loss: 0.8065175414085388, lr: 0.00028114897682949294\n",
      "Epoch 30 - Loss: 0.5814651250839233, lr: 0.0002901000669333403\n",
      "Epoch 40 - Loss: 0.5223512053489685, lr: 0.00029199347096895356\n",
      "Epoch 50 - Loss: 0.5204007029533386, lr: 0.0002920526206946322\n",
      "Epoch 60 - Loss: 0.513866126537323, lr: 0.000292249228980408\n",
      "Epoch 70 - Loss: 0.5149477124214172, lr: 0.0002922168524116694\n",
      "Epoch 80 - Loss: 0.5115444660186768, lr: 0.0002923185044998152\n",
      "Epoch 90 - Loss: 0.5073727965354919, lr: 0.0002924422209617735\n",
      "Epoch 100 - Loss: 0.5053799152374268, lr: 0.0002925009771626279\n",
      "Epoch 110 - Loss: 0.5058587789535522, lr: 0.0002924868791940556\n",
      "Epoch 120 - Loss: 0.5049082040786743, lr: 0.00029251485194386177\n",
      "Epoch 130 - Loss: 0.5042964816093445, lr: 0.0002925328263344626\n",
      "Epoch 140 - Loss: 0.5051950216293335, lr: 0.0002925064170636974\n",
      "Accuracy: 65.430%, time: 11.926s\n",
      "Epoch 0 - Loss: 1.440382957458496, lr: 0.00024265055985887304\n",
      "Epoch 10 - Loss: 0.6950411200523376, lr: 0.00028592239241594905\n",
      "Epoch 20 - Loss: 0.524667501449585, lr: 0.0002919229514082775\n",
      "Epoch 30 - Loss: 0.5462906360626221, lr: 0.0002912501544671042\n",
      "Epoch 40 - Loss: 0.5141794681549072, lr: 0.00029223985606180827\n",
      "Epoch 50 - Loss: 0.5129692554473877, lr: 0.00029227602640904314\n",
      "Epoch 60 - Loss: 0.5107276439666748, lr: 0.0002923428054640389\n",
      "Epoch 70 - Loss: 0.5063422918319702, lr: 0.0002924726312644757\n",
      "Epoch 80 - Loss: 0.5056387782096863, lr: 0.0002924933577192233\n",
      "Epoch 90 - Loss: 0.5044116973876953, lr: 0.0002925294425305153\n",
      "Epoch 100 - Loss: 0.5038447976112366, lr: 0.0002925460847619205\n",
      "Epoch 110 - Loss: 0.5046400427818298, lr: 0.0002925227339845774\n",
      "Epoch 120 - Loss: 0.5039501190185547, lr: 0.0002925429942547591\n",
      "Epoch 130 - Loss: 0.5031089782714844, lr: 0.00029256765891991216\n",
      "Epoch 140 - Loss: 0.5041146874427795, lr: 0.0002925381639783028\n",
      "Accuracy: 72.070%, time: 10.528s\n",
      "Epoch 0 - Loss: 4.27501106262207, lr: 1.529540494645553e-05\n",
      "Epoch 10 - Loss: 0.9186789989471436, lr: 0.00027569718959446116\n",
      "Epoch 20 - Loss: 0.637652575969696, lr: 0.00028812121424720886\n",
      "Epoch 30 - Loss: 0.5739843845367432, lr: 0.0002903504104891671\n",
      "Epoch 40 - Loss: 0.534485399723053, lr: 0.0002916207101845869\n",
      "Epoch 50 - Loss: 0.5196638107299805, lr: 0.0002920749117075636\n",
      "Epoch 60 - Loss: 0.5157971382141113, lr: 0.0002921913793573355\n",
      "Epoch 70 - Loss: 0.5118368864059448, lr: 0.000292309795698793\n",
      "Epoch 80 - Loss: 0.5055752396583557, lr: 0.0002924952282793738\n",
      "Epoch 90 - Loss: 0.506426990032196, lr: 0.0002924701340643409\n",
      "Epoch 100 - Loss: 0.5058346390724182, lr: 0.0002924875901920322\n",
      "Epoch 110 - Loss: 0.5037798881530762, lr: 0.00029254798912690445\n",
      "Epoch 120 - Loss: 0.5063057541847229, lr: 0.000292473708397946\n",
      "Epoch 130 - Loss: 0.5043500661849976, lr: 0.00029253125268720635\n",
      "Epoch 140 - Loss: 0.5045299530029297, lr: 0.00029252596867199583\n",
      "Accuracy: 72.852%, time: 10.074s\n",
      "Epoch 0 - Loss: 4.1680145263671875, lr: 2.0032719632379458e-05\n",
      "Epoch 10 - Loss: 1.2291090488433838, lr: 0.00025744908069104443\n",
      "Epoch 20 - Loss: 0.9449532628059387, lr: 0.0002743288134325067\n",
      "Epoch 30 - Loss: 0.6613868474960327, lr: 0.00028723346096743997\n",
      "Epoch 40 - Loss: 0.5409145355224609, lr: 0.0002914198703373993\n",
      "Epoch 50 - Loss: 0.5790421962738037, lr: 0.00029018148959956047\n",
      "Epoch 60 - Loss: 0.5180732011795044, lr: 0.0002921229238654955\n",
      "Epoch 70 - Loss: 0.5122020840644836, lr: 0.0002922989127144702\n",
      "Epoch 80 - Loss: 0.5118489861488342, lr: 0.00029230943524334916\n",
      "Epoch 90 - Loss: 0.507870614528656, lr: 0.0002924275089072649\n",
      "Epoch 100 - Loss: 0.5063301920890808, lr: 0.00029247298797441376\n",
      "Epoch 110 - Loss: 0.50571608543396, lr: 0.00029249108150629024\n",
      "Epoch 120 - Loss: 0.5048484206199646, lr: 0.0002925166095041051\n",
      "Epoch 130 - Loss: 0.5045158863067627, lr: 0.00029252638193430237\n",
      "Epoch 140 - Loss: 0.5050954222679138, lr: 0.0002925093466582217\n",
      "Accuracy: 75.391%, time: 10.431s\n",
      "Epoch 0 - Loss: 2.675551414489746, lr: 0.00013348819858066688\n",
      "Epoch 10 - Loss: 0.8611024022102356, lr: 0.0002785755178438473\n",
      "Epoch 20 - Loss: 0.6375612020492554, lr: 0.0002881245727548085\n",
      "Epoch 30 - Loss: 0.5540874600410461, lr: 0.0002910011566728081\n",
      "Epoch 40 - Loss: 0.5256642699241638, lr: 0.00029189251230141345\n",
      "Epoch 50 - Loss: 0.5122527480125427, lr: 0.000292297402323945\n",
      "Epoch 60 - Loss: 0.5086608529090881, lr: 0.0002924041263140064\n",
      "Epoch 70 - Loss: 0.506936252117157, lr: 0.0002924551107248802\n",
      "Epoch 80 - Loss: 0.5053496360778809, lr: 0.0002925018681614161\n",
      "Epoch 90 - Loss: 0.5049894452095032, lr: 0.00029251246323211866\n",
      "Epoch 100 - Loss: 0.5049558877944946, lr: 0.000292513449957038\n",
      "Epoch 110 - Loss: 0.5038945078849792, lr: 0.0002925446261623003\n",
      "Epoch 120 - Loss: 0.505565345287323, lr: 0.00029249551954691567\n",
      "Epoch 130 - Loss: 0.5041119456291199, lr: 0.00029253824446623665\n",
      "Epoch 140 - Loss: 0.5031890869140625, lr: 0.0002925653116197424\n",
      "Accuracy: 87.500%, time: 10.715s\n",
      "Accuracy: 74.648% ± 10.298%, time: 10.735s ± 0.631s\n"
     ]
    }
   ],
   "source": [
    "from requests import head\n",
    "from sklearn import metrics\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "import pickle \n",
    "import json\n",
    "from statistics import mean\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "patch = 4\n",
    "dim = 512\n",
    "depth = 3\n",
    "head = 4\n",
    "\n",
    "metrics_map = {}\n",
    "\n",
    "\n",
    "                \n",
    "model_info = {'patch': patch, 'dim': dim, 'depth': depth, 'head': head}\n",
    "\n",
    "model_name = json.dumps(model_info)\n",
    "print(model_name)\n",
    "v = get_model(patch, dim, depth, head)\n",
    "v = v.to(device)\n",
    "\n",
    "\n",
    "train_optimizer = Adam(v.parameters(), lr=3e-4)\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "\n",
    "train_scheduler = CosineLRScheduler(\n",
    "        train_optimizer,  t_initial=5\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "losses = np.array([])\n",
    "train_accuracies = np.array([])\n",
    "test_accuracies = np.array([])\n",
    "times = np.array([])\n",
    "for seed in range(5):\n",
    "    train_loader, val_loader = get_dataloader_transformer(cifar_data,cifar_data_val, n_workers, seed)\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        average_loss, loss, accuracy = training_epoch_transformer(v, train_loader, train_optimizer, criterion)\n",
    "\n",
    "        train_scheduler.step(average_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} - Loss: {average_loss}, lr: {train_optimizer.param_groups[0]['lr']}\")\n",
    "        losses = np.append(losses,loss)\n",
    "        train_accuracies = np.append(train_accuracies,accuracy)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    times = np.append(times,(start.elapsed_time(end)/1000))\n",
    "    #wait for everything to finish running\n",
    "    test_accuracy = evaluate(\n",
    "        v, val_loader\n",
    "    )\n",
    "    print(\n",
    "        f\"Accuracy: {100* np.array(test_accuracy).mean():.3f}%, time: {start.elapsed_time(end)/1000:.3f}s\"\n",
    "    )\n",
    "    test_accuracies = np.append(test_accuracies,test_accuracy)\n",
    "\n",
    "print(\n",
    "        f\"Accuracy: {100* np.array(test_accuracies).mean():.3f}% \\u00B1 {100*np.array(test_accuracies).std():.3f}%, time: {np.array(times).mean():.3f}s \\u00B1 {np.array(times).std():.3f}s\"\n",
    "    )\n",
    "metrics_map[model_name] = {'losses': losses, 'train_accuracies': train_accuracies, 'test_accuracies': test_accuracies, 'times': times}\n",
    "with open('metrics-vit.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics_map, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp691",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
