{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch.nn.functional as F\n",
    "def train(model, device, train_loader, optimizer,  epoch,criterion=F.cross_entropy, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MobileNetV3\n",
      "seed 0\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.949406\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.039415\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.019079\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.012787\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.009656\n",
      "\n",
      "Test set: Average loss: 0.9493, Accuracy: 262/400 (65.50%)\n",
      "\n",
      "seed 1\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 7.406858\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.054617\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.020571\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.012888\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.009409\n",
      "\n",
      "Test set: Average loss: 0.7361, Accuracy: 289/400 (72.25%)\n",
      "\n",
      "seed 2\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 3.668649\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.030636\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.014763\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.009904\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.007470\n",
      "\n",
      "Test set: Average loss: 0.4910, Accuracy: 325/400 (81.25%)\n",
      "\n",
      "seed 3\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 5.203844\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.035992\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.016781\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.011123\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.008354\n",
      "\n",
      "Test set: Average loss: 0.8121, Accuracy: 289/400 (72.25%)\n",
      "\n",
      "seed 4\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 6.753372\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.047046\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.019091\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.012231\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.009058\n",
      "\n",
      "Test set: Average loss: 0.9430, Accuracy: 259/400 (64.75%)\n",
      "\n",
      "seed 5\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 6.069539\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.042773\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.018183\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.011780\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.008738\n",
      "\n",
      "Test set: Average loss: 0.8857, Accuracy: 266/400 (66.50%)\n",
      "\n",
      "seed 6\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 4.237350\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.036656\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.017155\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.011365\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.008546\n",
      "\n",
      "Test set: Average loss: 0.7458, Accuracy: 299/400 (74.75%)\n",
      "\n",
      "seed 7\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 3.258659\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.027103\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.013524\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.009121\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.006918\n",
      "\n",
      "Test set: Average loss: 0.6162, Accuracy: 315/400 (78.75%)\n",
      "\n",
      "seed 8\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.890091\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.026032\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.012947\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.008749\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.006638\n",
      "\n",
      "Test set: Average loss: 0.5421, Accuracy: 333/400 (83.25%)\n",
      "\n",
      "seed 9\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 7.424995\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.053600\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.020709\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.013065\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.009600\n",
      "\n",
      "Test set: Average loss: 0.9287, Accuracy: 258/400 (64.50%)\n",
      "\n",
      "Acc over 5 instances: 72.38 +- 6.67 , time: 14.79\n",
      "Acc over 5 instances: 72.38 +- 6.67 , time:  14.79 +- 0.58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "torch.cuda.benchmark = True\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
    "resize = transforms.Resize(224)\n",
    "\n",
    "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device) # you will really need gpu's for this part\n",
    "\n",
    "##### Cifar Data\n",
    "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "\n",
    "#We need two copies of this due to weird dataset api\n",
    "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "\n",
    "accs = []\n",
    "\n",
    "\n",
    "mobilenet_v3_large = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=10)\n",
    "\n",
    "classfiers = [ mobilenet_v3_large]\n",
    "\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from timm.optim import SGDP\n",
    "\n",
    "def run(model):\n",
    "    print(model.__class__.__name__)\n",
    "    accs = np.array([])\n",
    "    times = np.array([])\n",
    "    losses = np.array([])   \n",
    "    model.to(device)\n",
    "    optimizer = SGDP(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "    \n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial=5)\n",
    "    for seed in range(10):\n",
    "        print(\"seed\", seed)\n",
    "        prng = RandomState(seed)\n",
    "        random_permute = prng.permutation(np.arange(0, 5000))\n",
    "        classes =  prng.permutation(np.arange(0,10))\n",
    "        indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
    "\n",
    "        train_data = Subset(cifar_data, indx_train)\n",
    "        val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "        print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                                    batch_size=128,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                                batch_size=128,\n",
    "                                                shuffle=False)\n",
    "        \n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        for epoch in range(50):\n",
    "            loss = train(model, device, train_loader, optimizer, epoch, display=epoch%10==0)\n",
    "            losses = np.append(losses, loss)\n",
    "            scheduler.step(loss)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        time = start.elapsed_time(end)\n",
    "        times = np.append(times, time)        \n",
    "        acc = test(model, device, val_loader)\n",
    "        accs = np.append(accs, acc)\n",
    "    print('Acc over 5 instances: %.2f +- %.2f , time: %.2f'%(accs.mean(),accs.std(), times.mean()/1000))    \n",
    "    return accs, times, losses\n",
    "\n",
    "metrics_map = {}\n",
    "for model in classfiers:\n",
    "    accs, times, losses = run(model)\n",
    "    print('Acc over 5 instances: %.2f +- %.2f , time:  %.2f +- %.2f'%(accs.mean(),accs.std(), times.mean()/1000, times.std()/1000))\n",
    "    metrics_map[model.__class__.__name__] = (accs, times, losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp691",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
